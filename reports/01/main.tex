% PP-Article.tex for AEA last revised 22 June 2011
\documentclass[twocolumn, a4paper]{article}

%%%%%% NOTE FROM OVERLEAF: The mathtime package is no longer publicly available nor distributed. We recommend using a different font package e.g. mathptmx if you'd like to use a Times font.
\usepackage{mathptmx}
\usepackage{amsmath}
\usepackage[dutch]{babel}
\usepackage{subcaption}
\usepackage[width=.8\textwidth]{caption}
\usepackage{float}
\usepackage{booktabs}
\usepackage{multicol}
\usepackage{longtable}
\usepackage{minted}
% If you have trouble with the mathtime package please see our technical support 
% document at: http://www.aeaweb.org/templates/technical_support.pdf
% You may remove the mathtime package if you can't get it working but your page
% count may be inaccurate as a result.
% \usepackage[cmbold]{mathtime}
\usepackage{xargs}                      % Use more than one optional parameter in a new commands 
\usepackage[pdftex,dvipsnames]{xcolor}  % Coloured text etc.
% 
\usepackage{pdfpages}
\usepackage[colorinlistoftodos,prependcaption,textsize=tiny]{todonotes}
\newcommandx{\unsure}[2][1=]{\todo[linecolor=red,backgroundcolor=red!25,bordercolor=red,#1]{#2}}
\setlength{\marginparwidth}{2cm}
% Note: you may use either harvard or natbib (but not both) to provide a wider
% variety of citation commands than latex supports natively. See below.

% Uncomment the next line to use the natbib package with bibtex 
%\usepackage{natbib}
\usepackage{titlesec}

\titlespacing*\section{0pt}{12pt plus 4pt minus 2pt}{0pt plus 2pt minus 2pt}
\titlespacing*\subsection{0pt}{12pt plus 4pt minus 2pt}{0pt plus 2pt minus 2pt}
\titlespacing*\subsubsection{0pt}{12pt plus 4pt minus 2pt}{0pt plus 2pt minus 2pt}



% Uncomment the next line to use the harvard package with bibtex
%\usepackage[abbr]{harvard}

% This command determines the leading (vertical space between lines) in draft mode
% with 1.5 corresponding to "double" spacing.
\begin{document}

\title{Geavanceerde computerarchitectuur: Labo 01 \\ 
\large{Array omwisselen}}
\author{\textsc{Anton Danneels en Pieter Delobelle}}
\date{}
\maketitle


\begin{figure*}[h]
  \centering
  \includegraphics[width=0.72\textwidth]{cpu_vs_gpu.eps}
  \caption{Eerste experiment, de vergelijking tussen CPU en GPU voor verschillende array-groottes met een blocksize $n=64$}
  \label{fig:b4_cpu_gpu}
\end{figure*}

\section{Inleiding}
Het doel van dit labo is om een array om te draaien, zowel op de CPU alsook de GPU van de computer. 
Hierdoor moet een inzicht bekomen worden over het performantieverschil dat een GPU kan bieden ten opzichte van een CPU; en in welke situaties dit verschil naar boven komt.

Deze opdracht wordt ge\"implementeerd op een grafische kaart van NVIDEA, waarbij het \emph{CUDA}-framework gebruikt kan worden. 

\section{Analyse}

\subsection{GPU}
Een GPU bestaat uit vele CUDA-cores, bedoeld om een \emph{single instruction multiple thread} (SIMT) architectuur te ondersteunen. Deze cores zijn verdeeld over een aantal \emph{stream multiprocessors} (SM), welke ook weer opgedeeld zijn in een aantal \emph{wraps}. Deze wraps kunnen typisch 32 threads in parallel runnen, waardoor de blocksize idealiter een veelvoud van 32 is.

\subsection{CUDA}
Om de taken van de threads effici\"ent te kunnen verdelen, biedt CUDA een \emph{grid} aan. Dit grid bestaat uit blokken, die dus uitgevoerd worden door een SM. Binnen deze blokken kunnen threads ge\"identificeerd worden door een 1D, 2D of 3D set van indices. In ieder geval, de grootte $x \cdot y \cdot z$ kan niet groter zijn dan het aantal threads op een SM.

Om dit op te lossen, zijn blokken ook ge-indexed, waarbij een queue wordt aangelegd door de GPU. Om de algemene index te bepalen, wordt de volgende code gebruikt (voor 1 dimensie): 

\begin{minted}{c}
  int i = blockIdx.x * blockDim.x 
    + threadIdx.x;
\end{minted}
\label{c:index}

\subsection{XOR-verwisseling}
Om de waarden te swappen, wordt gebruikt gemaakt van het \emph{XOR swapping algoritme}. Dit werkt als volgt:

\begin{minted}{c}
  X = X XOR Y
  Y = Y XOR X
  X = X XOR Y
\end{minted}

Het resultaat zijn twee variabelen die van waarde zijn verwisseld, zonder gebruik te maken van een helper-variabele.

\section{Oplossing}
Om de GPU en CPU te kunnen vergelijken, is de code om een array te swappen twee keer ge\"implementeerd: eenmaal voor de CPU met een traditionele for-lus en eenmaal voor de GPU, waarbij met variabele blocksize.

Daarnaast is ook de lengte van de array variabel, waardoor de evaluatie over meerdere lengtes kan gebeuren. De code hiervoor is te vinden als bijlage.


\subsection{Swap op de CPU}
Voor de array om te wisselen op de CPU, itereren we tot het midden van de array, waarbij beide helften worden omgewisseld.

\begin{figure*}
  \centering
  \includegraphics[width=0.65\textwidth]{blocksize.eps}
  \caption{Invloed van de blocksize op de uitvoersnelheid.}
  \label{blocksize}
\end{figure*}
  
\begin{figure*}
    \centering
    \includegraphics[width=1.03\textwidth]{overhead.eps}
    \caption{Overhead van het kopieren en initialiseren van de data naar de GPU ten opzichte van de daadwerkelijke operatie uitvoeren op de GPU.}
    \label{fig:overhead}
\end{figure*}

\begin{minted}{console}
  0000,0001,0002,0003,0004,0005
  0005,0004,0003,0002,0001,0000 
  Operation in 0 ms
\end{minted}

Om de performantie te vergelijken, benchmarken we deze operatie ook. Dit doen we voor de CPU door \mintinline{c}{clock()} twee maal op te roepen. Deze klok heeft technisch gezien een precisie van 1 ns, dus de variabele \mintinline{c}{CLOCKS_PER_SEC} = 1 000 000. Door een geheelgetallige afronding is de precisie van onze meting afgekapt op 1 ms. Voor kleine arrays is dit niet ideaal.

Deze timing is onderverdeeld in de tijd nodig om de array te initialiseren en de daadwerkelijke tijd nodig om de wisseling uit te voeren.

\subsection{Swap op de GPU}
De swap-operatie op de GPU is inhoudelijk gelijk, maar de lus is vervangen door functie in sectie~\ref{c:index} om de index te bepalen. Deze code is in een kernel beschreven, zoals te zien is in de broncode in bijlage.

De benchmarks worden bij CUDA niet gemeten via de \mintinline{c}{clock()}-functie, aangezien we er niet vanuit mogen gaan dat de code op de GPU niet gesynchroniseerd hoeft te zijn. CUDA biedt echter ook een functie aan, waarmee de tijd als zwevende kommagetal kan gemeten worden. 

\begin{minted}{c}
  cudaEventRecord(swap_start);
  cudaEventSynchronize(swap_stop);
  cudaEventElapsedTime( &ms, 
    swap_start, 
    swap_stop );
\end{minted}
 
Zoals bij de CPU, vergelijken we zowel de daadwerkelijke operatie alsook de tijd die nodig is om de array te initialiseren Ã©n bijkomend ook te kopieren naar het geheugen op de videokaart.

In het eerste experiment, dat te zien is op figuur~\ref{fig:b4_cpu_gpu}, is de blocksize $n$ vast gedefineerd, terwijl iteratief een range van array-groottes wordt ge\"evalueerd. De resultaten hiervan---en ook de tweede test--- zijn ook te vinden in de bijlagen.

Een tweede experiment richt specifiek op de blocksize, waarvan de resultaten in figuur~\ref{blocksize} zijn weergegevene en ter vergelijking ook de CPU-tijd. 

\subsection{Vergelijking}
De resultaten van het eerste experiment in figuur ~\ref{fig:b4_cpu_gpu} vergelijken de swap- en totale tijden tussen de CPU en GPU. Hierbij is de CPU als baseline (100 \%) genomen.

Uit de data valt duidelijk op te maken dat de GPU sneller is voor

\paragraph{blocksize}

In figuur~\ref{blocksize} is de invloed van de grootte van de blocksize op de uitvoersnelheid weergegeven. Ter vergelijking is ook de tijd op de CPU weergegeven.

Uit de grafiek is duidelijk op te maken dat blocksizes $n$ die een veelvoud zijn van 32 een overduidelijk sneller zijn. Hoe beter de wraps benut worden---door dus veelvouden van 32 te kiezen, hoe beter de uitvoersnelheid is. Door deze zeer ongelukkig te kiezen, zien we zelfs dat de resultaten zelfs slechter zijn dan een uitvoering op de CPU. 



\paragraph{Overhead}
Een derde conclusie die uit de data getrokken kan worden, is te zien in figuur~\ref{fig:overhead}. Hierop is de \emph{overhead} gevisualiseerd. Concreet betekend dit hoeveel tijd gespendeerd wordt aan het kopieren van de array naar de GPU en terug ten opzichte van de tijd die nodig is om de operatie zelf uit te voeren. Het spreekt voor zich dat een lagere---relatieve maar uiteraard ook absolute---overhead positief is. 

Bij kleine taken is goed te zien dat de overhead zeer groot is, tot meer dan 1000\%. Dit kan beschouwd worden als een penalty voor kleine datasets. Hiervoor kan het interessanter zijn om de CPU te gebruiken, of indien de toepassing het toelaat een pipeline op te zetten waardoor deze overhead niet bij iedere operatie nodig is. 

\section{Besluit}
Bij grote datasets is de GPU zoals verwacht sneller. De performantie is echter afhankelijk van een aantal factoren. Ten eerste moet de blocksize als veelvoud van 32 gekozen worden, om de wraps optimaal te benutten. Door dit niet te doen wordt de GPU onderbenut.

Een tweede factor is de grootte van de data waarop operaties moeten uitgevoerd worden. Kleine datasets hebben een grotere overhead met het overbrengen van deze data, waarvoor de CPU mogelijks alsnog een betere keuze is.


\onecolumn

\appendix

\begin{figure}[H]
  \centering
  \includegraphics[width=0.4\textwidth]{flowgraph.pdf}
  \caption{Flowchart van de code om de array te swappen op de GPU.}
\end{figure}

\newpage

\include{data}

\newpage

\inputminted[tabsize=4,obeytabs, ]{c}{main.c}

% The appendix command is issued once, prior to all appendices, if any.
%\appendix
\end{document}

